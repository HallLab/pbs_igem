{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: QC NHANES Data\n",
    "\n",
    "Documentation: https://halllab.atlassian.net/wiki/spaces/IGEM/pages/79233025/Phase+5+QC+NHANES+Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpy2 ModuleSpec(name='rpy2', loader=<_frozen_importlib_external.SourceFileLoader object at 0x35464df00>, origin='/Users/andrerico/Works/Projects/pbs_igem/.venv/lib/python3.10/site-packages/rpy2/__init__.py', submodule_search_locations=['/Users/andrerico/Works/Projects/pbs_igem/.venv/lib/python3.10/site-packages/rpy2'])\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import igem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path to the data folder\n",
    "path = Path().resolve()\n",
    "path_data = path / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 101,316 observations of 917 variables\n",
      "Fields to run QC: 1140 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrerico/Works/Projects/pbs_igem/.venv/lib/python3.10/site-packages/igem/epc/clarite/load/load.py:77: DtypeWarning: Columns (442,869) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return clarite.load.from_csv(filename, index_col, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Reading the NHANES data with normalized medications\n",
    "df_nhanes = igem.epc.load.from_csv(\n",
    "    str(path_data) + \"/step_04_02_nhanes_data_with_medications.csv\"\n",
    "    )\n",
    "# print(f\"Start QC Process with: {len(df_nhanes)} records\")\n",
    "\n",
    "# Reading the fields to run the QC\n",
    "ls_fields = pd.read_csv(\n",
    "    (str(path_data) + \"/step_02_07_Fields.csv\"),\n",
    "    )['field_name'].tolist()\n",
    "\n",
    "# add the normalized medication fields\n",
    "ls_fields.extend(['ID', 'Cycle', 'LBDLDL_N', 'LBXTC_N'])\n",
    "print(f\"Fields to run QC: {len(ls_fields)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 05_00: Columns Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syncing the fields to the NHANES data\n",
    "existing_fields = [field for field in ls_fields if field in df_nhanes.columns]\n",
    "df_nhanes = df_nhanes[existing_fields]\n",
    "\n",
    "# Droping the rows with missing values\n",
    "df_nhanes = df_nhanes.dropna(axis=1, how='all')\n",
    "\n",
    "# Droping duplicated columns\n",
    "df_nhanes = df_nhanes.loc[:, ~df_nhanes.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change DUMMY values to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace specific values with NaN in a column\n",
    "def replace_values_with_nan(df, column_name, values_to_replace):\n",
    "    if column_name in df.columns:\n",
    "        df[column_name] = df[column_name].replace(values_to_replace, np.nan)\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace Binary Fields with values 77 and 99 to NaN\n",
    "column_list = ['SLD010H',]\n",
    "values_to_replace = [77, 99]\n",
    "for column_name in column_list:\n",
    "    df_nhanes = replace_values_with_nan(df_nhanes, column_name, values_to_replace)\n",
    "\n",
    "# Replace Binary Fields with values 7 and 9 to NaN\n",
    "column_list = ['SMQ770',]\n",
    "values_to_replace = [7, 9]\n",
    "for column_name in column_list:\n",
    "    df_nhanes = replace_values_with_nan(df_nhanes, column_name, values_to_replace)\n",
    "\n",
    "# Add here if you want to replace other values with NaN\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan,  1.,  3.,  5.,  2.,  4.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the unique values of the column\n",
    "df_nhanes['SMQ770'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 05_01: Filter Target Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 18: 42112\n",
      ">= 18: 59204\n",
      "Total: 101316\n"
     ]
    }
   ],
   "source": [
    "# Slip data into two groups (age < 18 and age >= 18)\n",
    "# Many records do not have age information, so we will drop them (sequence > 0)\n",
    "df_nhanes_lt = df_nhanes.loc[df_nhanes[\"RIDAGEYR\"] < 18]\n",
    "df_nhanes_gt = df_nhanes.loc[df_nhanes[\"RIDAGEYR\"] >= 18]\n",
    "\n",
    "print(f'< 18: {len(df_nhanes_lt)}')\n",
    "print(f'>= 18: {len(df_nhanes_gt)}')\n",
    "print(f'Total: {len(df_nhanes_lt)+len(df_nhanes_gt)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After drop NA in all Cofounders: 55206 records\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing values in the cofounders\n",
    "columns_to_check = ['RIDAGEYR', 'RIAGENDR', 'RIDRETH1', 'BMXBMI']\n",
    "df_nhanes_gt = df_nhanes_gt.dropna(subset=columns_to_check)\n",
    "print(f\"After drop NA in all Cofounders: {len(df_nhanes_gt)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 05_02: Categorize Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running categorize\n",
      "--------------------------------------------------------------------------------\n",
      "96 of 834 variables (11.51%) are classified as constant (1 unique value).\n",
      "199 of 834 variables (23.86%) are classified as binary (2 unique values).\n",
      "106 of 834 variables (12.71%) are classified as categorical (3 to 6 unique values).\n",
      "342 of 834 variables (41.01%) are classified as continuous (>= 15 unique values).\n",
      "58 of 834 variables (6.95%) were dropped.\n",
      "\t58 variables had zero unique values (all NA).\n",
      "33 of 834 variables (3.96%) were not categorized and need to be set manually.\n",
      "\t30 variables had between 6 and 15 unique values\n",
      "\t3 variables had >= 15 values but couldn't be converted to continuous (numeric) values\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# # Categories the columns types\n",
    "df_nhanes_gt = igem.epc.modify.categorize(df_nhanes_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of object columns:  33 \n",
      "\n",
      "The list of object columns: \n",
      "Index(['SMD100BR', 'SMQ143', 'SMQ078', 'SMQ621', 'SMQ660', 'SSDCP', 'LBXV1D',\n",
      "       'LBXV2T', 'DBD195', 'DBQ197', 'DBD235A', 'DBD235AE', 'DBD235B',\n",
      "       'DBD235C', 'DBD235CE', 'CBQ050', 'CBQ800', 'LBXBFOA', 'GTDSCMMN',\n",
      "       'LBX10AL', 'URXPTU', 'LBDBGESI', 'SLD010H', 'SLQ300', 'SLQ320',\n",
      "       'DSQTCAFF', 'DUQ272', 'DUQ352', 'SMQ750', 'ALQ142', 'SMD780', 'URXMET',\n",
      "       'Cycle'],\n",
      "      dtype='object') \n",
      "\n",
      "One file was create with the Object columns: \n",
      "/Users/andrerico/Works/Projects/pbs_igem/data/step_05_01_Object_Columns.csv\n"
     ]
    }
   ],
   "source": [
    "# Information about the columns not categorized and need to be set manually\n",
    "ls_object_columns = df_nhanes_gt.select_dtypes(include=['object']).columns\n",
    "print(\n",
    "    f\"Number of object columns:  {len(ls_object_columns)} \\n\" \n",
    ")\n",
    "\n",
    "print(\"The list of object columns: \")\n",
    "print(f\"{ls_object_columns} \\n\")\n",
    "\n",
    "df_nhanes_gt.select_dtypes(include=['object']).to_csv(str(path_data) + '/step_05_01_Object_Columns.csv')\n",
    "print(f\"One file was create with the Object columns: \")\n",
    "print(f\"{str(path_data) + '/step_05_01_Object_Columns.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running make_categorical\n",
      "--------------------------------------------------------------------------------\n",
      "Set 20 of 773 variable(s) as categorical, each with 55,206 observations\n",
      "================================================================================\n",
      "================================================================================\n",
      "Running make_continuous\n",
      "--------------------------------------------------------------------------------\n",
      "Set 9 of 773 variable(s) as continuous, each with 55,206 observations\n",
      "================================================================================\n",
      "================================================================================\n",
      "Running make_continuous\n",
      "--------------------------------------------------------------------------------\n",
      "Set 2 of 773 variable(s) as continuous, each with 55,206 observations\n",
      "================================================================================\n",
      "================================================================================\n",
      "Running colfilter\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid columns passed to 'skip': SLQ300, SLQ320, SMD100BR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 34\u001b[0m\n\u001b[1;32m     28\u001b[0m df_nhanes_gt \u001b[38;5;241m=\u001b[39m igem\u001b[38;5;241m.\u001b[39mepc\u001b[38;5;241m.\u001b[39mmodify\u001b[38;5;241m.\u001b[39mmake_continuous(\n\u001b[1;32m     29\u001b[0m     df_nhanes_gt,\n\u001b[1;32m     30\u001b[0m     only\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSLD010H\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMQ770\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Text fields that will be dropped\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m df_nhanes_gt \u001b[38;5;241m=\u001b[39m \u001b[43migem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodify\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_nhanes_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSLQ300\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSLQ320\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSMD100BR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Global Variables List to keep all Fields Reviewed and set to corret type\u001b[39;00m\n\u001b[1;32m     40\u001b[0m global_vars \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSLD010H\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMQ770\u001b[39m\u001b[38;5;124m'\u001b[39m, ]\n",
      "File \u001b[0;32m~/Works/Projects/pbs_igem/.venv/lib/python3.10/site-packages/igem/epc/clarite/modify/modify.py:110\u001b[0m, in \u001b[0;36mcolfilter\u001b[0;34m(data, skip, only)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolfilter\u001b[39m(\n\u001b[1;32m     74\u001b[0m     data,\n\u001b[1;32m     75\u001b[0m     skip: Optional[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m     only: Optional[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     77\u001b[0m ):\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    Remove some variables (skip) or keep only certain variables (only)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    ================================================================================\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     df_result \u001b[38;5;241m=\u001b[39m \u001b[43mclarite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodify\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_result\n",
      "File \u001b[0;32m~/Works/Projects/pbs_igem/.venv/lib/python3.10/site-packages/clarite/internal/utilities.py:19\u001b[0m, in \u001b[0;36mprint_wrap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m click\u001b[38;5;241m.\u001b[39mecho(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m click\u001b[38;5;241m.\u001b[39mecho(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m click\u001b[38;5;241m.\u001b[39mecho(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Works/Projects/pbs_igem/.venv/lib/python3.10/site-packages/clarite/modules/modify.py:216\u001b[0m, in \u001b[0;36mcolfilter\u001b[0;34m(data, skip, only)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;129m@print_wrap\u001b[39m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolfilter\u001b[39m(\n\u001b[1;32m    181\u001b[0m     data,\n\u001b[1;32m    182\u001b[0m     skip: Optional[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    183\u001b[0m     only: Optional[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    184\u001b[0m ):\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    Remove some variables (skip) or keep only certain variables (only)\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    ================================================================================\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     boolean_keep \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_skip_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     dtypes \u001b[38;5;241m=\u001b[39m _get_dtypes(data)\n\u001b[1;32m    218\u001b[0m     click\u001b[38;5;241m.\u001b[39mecho(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeeping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mboolean_keep\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Works/Projects/pbs_igem/.venv/lib/python3.10/site-packages/clarite/internal/utilities.py:73\u001b[0m, in \u001b[0;36m_validate_skip_only\u001b[0;34m(data, skip, only)\u001b[0m\n\u001b[1;32m     71\u001b[0m     invalid_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(skip) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(data))\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(invalid_cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid columns passed to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m     columns \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;241m~\u001b[39mdata\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(skip), index\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m skip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m only \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid columns passed to 'skip': SLQ300, SLQ320, SMD100BR"
     ]
    }
   ],
   "source": [
    "# Set the cycle column to a categorical variable and order it\n",
    "cycle_order = [\n",
    "    '1999-2000',\n",
    "    '2001-2002',\n",
    "    '2003-2004',\n",
    "    '2005-2006',\n",
    "    '2007-2008',\n",
    "    '2009-2010',\n",
    "    '2011-2012',\n",
    "    '2013-2014',\n",
    "    '2015-2016',\n",
    "    '2017-2018'\n",
    "]\n",
    "df_nhanes_gt['Cycle'] = pd.Categorical(df_nhanes_gt['Cycle'], categories=cycle_order, ordered=True)\n",
    "\n",
    "# Objects Types to Categorical Types\n",
    "df_nhanes_gt = igem.epc.modify.make_categorical(\n",
    "    df_nhanes_gt,\n",
    "    only=['SMQ143','SMQ078','SMQ621','SMQ660','DBD195','DBQ197','DBD235A','DBD235AE','DBD235B','DBD235C','DBD235CE','CBQ050','CBQ800','GTDSCMMN','SLD010H','DUQ272','DUQ352','SMQ750','ALQ142','SMD780'])\n",
    "\n",
    "# Objects Types to Continuous Types\n",
    "df_nhanes_gt = igem.epc.modify.make_continuous(\n",
    "    df_nhanes_gt,\n",
    "    only=['SSDCP','LBXV1D','LBXV2T','LBXBFOA','LBX10AL','URXPTU','LBDBGESI','DSQTCAFF','URXMET']\n",
    "    )\n",
    "\n",
    "# Categorical Types to Continuous Types\n",
    "df_nhanes_gt = igem.epc.modify.make_continuous(\n",
    "    df_nhanes_gt,\n",
    "    only=['SLD010H', 'SMQ770']\n",
    "    )\n",
    "\n",
    "# Text fields that will be dropped\n",
    "df_nhanes_gt = igem.epc.modify.colfilter(\n",
    "    df_nhanes_gt,\n",
    "    skip=['SLQ300','SLQ320','SMD100BR',]\n",
    "    )\n",
    "\n",
    "# Global Variables List to keep all Fields Reviewed and set to corret type\n",
    "global_vars = ['SLD010H', 'SMQ770', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of object columns:  0 \n",
      "\n",
      "55,206 observations of 773 variables\n",
      "\t199 Binary Variables\n",
      "\t126 Categorical Variables\n",
      "\t352 Continuous Variables\n",
      "\t0 Unknown-Type Variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if exist any Object Data Type\n",
    "ls_object_columns = df_nhanes_gt.select_dtypes(include=['object']).columns\n",
    "print(\n",
    "    f\"Number of object columns:  {len(ls_object_columns)} \\n\" \n",
    ")\n",
    "\n",
    "igem.epc.describe.summarize(df_nhanes_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 05_03: Phenotypes and Exposures QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 05_03_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the groups\n",
    "#  Outcomes = Phenotypes\n",
    "#  Covariants = Cofounders\n",
    "#  Variants = Exposures\n",
    "# TODO: Set this list in a parameter file\n",
    "list_covariats = ['RIDAGEYR', 'RIAGENDR', 'RIDRETH1', 'BMXBMI', 'Cycle']\n",
    "list_outcomes_wo_adj = ['LBDLDL', 'LBXTC']\n",
    "list_outcomes = ['LBDLDL_N', 'LBXTC_N', 'LBXSTR', 'LBDHDL', 'LBXHDD', 'LBDHDD']\n",
    "\n",
    "# Define the list of exposes\n",
    "excluded_columns = set(list_covariats + list_outcomes + list_outcomes_wo_adj)\n",
    "list_variants = [col for col in df_nhanes_gt.columns if col not in excluded_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 05_04: Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 05_04_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_columns (df1, df2):\n",
    "    # Syncs for replication and discovery datasets to have the same columns\n",
    "    # get the common columns\n",
    "    common_columns = df1.columns.intersection(df2.columns)\n",
    "\n",
    "    # filter both DataFrames to keep only the common columns\n",
    "    df1 = df1[common_columns]\n",
    "    df2 = df2[common_columns]\n",
    "\n",
    "    # check if both groups as the same number of columns\n",
    "    n_df1 = len(df1.columns)\n",
    "    n_df2 = len(df2.columns)\n",
    "\n",
    "    if n_df1 != n_df2:\n",
    "        # Raise an error if the number of columns is different\n",
    "        print(f\"---> DF1 has {n_df1} columns and DF2 has {n_df2} columns\")\n",
    "        print(\"---> Columns in DF1 but not in DF2:\")\n",
    "        print(set(df1.columns) - set(df2.columns))\n",
    "        print(\"---> Columns in DF2 but not in DF1:\")\n",
    "        print(set(df2.columns) - set(df1.columns))\n",
    "        return False, df1, df2\n",
    "    \n",
    "    print(f\"---> {n_df1} columns and {len(df1)} rows on DF1/Discovery dataset\")\n",
    "    print(f\"---> {n_df2} columns and {len(df2)} rows on DF2/Replicate dataset\")\n",
    "    \n",
    "    return True, df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 05_04_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSyncError(Exception):\n",
    "    pass\n",
    "\n",
    "# Function to split datasets\n",
    "def split_datasets(\n",
    "        df,\n",
    "        outcome,\n",
    "        list_covariats,\n",
    "        list_variants,\n",
    "        split_col='Cycle',\n",
    "        split_value='2007-2008',\n",
    "        random_split=False,\n",
    "        discovery_percent=0.7\n",
    "        ):\n",
    "\n",
    "    try:\n",
    "        # Filter the DataFrames to keep only the columns of interest\n",
    "        df = df[[outcome] + list_covariats + list_variants].dropna(subset=[outcome])\n",
    "\n",
    "        if random_split:\n",
    "            # Random split based on the specified discovery percentage\n",
    "            df = df.sample(frac=1, random_state=42)  # Shuffle the DataFrame\n",
    "            # df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle the DataFrame\n",
    "            discovery_size = int(discovery_percent * len(df))\n",
    "            \n",
    "            df_discovery = df[:discovery_size]\n",
    "            df_replication = df[discovery_size:]\n",
    "        else:\n",
    "            # Split based on the split_value in split_col\n",
    "            df_discovery = df[df[split_col] <= split_value]\n",
    "            df_replication = df[df[split_col] > split_value]\n",
    "\n",
    "        # Drop columns with all NaN values in both DataFrames\n",
    "        df_discovery = df_discovery.dropna(axis=1, how='all')\n",
    "        df_replication = df_replication.dropna(axis=1, how='all')\n",
    "\n",
    "        # Syncs for replication and discovery datasets to have the same columns\n",
    "        check_integrid, df_discovery, df_replication = sync_columns(\n",
    "            df_discovery,\n",
    "            df_replication\n",
    "        )\n",
    "\n",
    "        if check_integrid:\n",
    "            return df_discovery, df_replication\n",
    "        else:\n",
    "            raise ColumnSyncError(\"Column synchronization failed between discovery and replication datasets.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 05_04_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC to Outcome Function\n",
    "def outcome_qc(df, outcome, group, plot=True):\n",
    "    df = df[[outcome]]\n",
    "\n",
    "    # Remove phenotypes with more than 90% missing values\n",
    "    df = igem.epc.modify.colfilter_percent_zero(df, filter_percent=90)\n",
    "\n",
    "    # Get skew value\n",
    "    skew_value = igem.epc.describe.skewness(\n",
    "        df,\n",
    "        dropna='True'\n",
    "        ).loc[0, 'skew']\n",
    "    \n",
    "    # Log transform all phenotypes since the skewness values are greater than abs(0.5)\n",
    "    if abs(skew_value) > 0.5:\n",
    "        df_log = igem.epc.modify.transform(df, 'log')\n",
    "        # Run skewness again after log transf\n",
    "        skew_value_log = igem.epc.describe.skewness(\n",
    "            df_log,\n",
    "            dropna='True'\n",
    "            ).loc[0, 'skew']\n",
    "        \n",
    "        if plot:\n",
    "            # # Plot the outcome distribution\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "            \n",
    "            # Plot original data\n",
    "            axes[0].hist(df[outcome].dropna(), bins=100, color='blue', alpha=0.7)\n",
    "            axes[0].set_title(f\"{group}: Skew to {outcome} = {skew_value:.6f}\")\n",
    "            axes[0].set_xlabel(outcome)\n",
    "            axes[0].set_ylabel('Frequency')\n",
    "\n",
    "            # Plot log-transformed data\n",
    "            axes[1].hist(df_log[outcome].dropna(), bins=100, color='green', alpha=0.7)\n",
    "            axes[1].set_title(f\"{group}: Skew to {outcome} after log transf = {skew_value_log:.6f}\")\n",
    "            axes[1].set_xlabel(outcome)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return df_log\n",
    "\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 05_04_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC to Variant Function\n",
    "def variant_qc(df, list_variants):\n",
    "    # Filtering the variant columns that exist in the discovery DataFrame\n",
    "    existing_variants = [var for var in list_variants if var in df.columns]\n",
    "    df_variants = df[existing_variants]\n",
    "\n",
    "    # Separate categorical and continuous variables\n",
    "    objecy_columns = df_variants.select_dtypes(include=['object']).columns\n",
    "    categorical_columns = df_variants.select_dtypes(include=['category']).columns\n",
    "    continuous_columns = df_variants.select_dtypes(include=['number']).columns\n",
    "    # print(f\"---> Number of object columns:  {len(objecy_columns)}\")\n",
    "    # print(f\"---> Number of categorical columns:  {len(categorical_columns)}\")\n",
    "    # print(f\"---> Number of continuous columns:  {len(continuous_columns)}\")\n",
    "\n",
    "\n",
    "    ### QC in Categorical Exposures\n",
    "    # Separate categorical and continuous variables\n",
    "    df_categorical = df_variants[categorical_columns]\n",
    "    df_continuous = df_variants[continuous_columns]\n",
    "\n",
    "\n",
    "    # Apply colfilter_min_n to categorical columns\n",
    "    df_categorical = igem.epc.modify.colfilter_min_n(df_categorical, n=200)\n",
    "    # Apply cat filter and colfilter_min_cat_n to categorical columns\n",
    "    df_categorical = igem.epc.modify.colfilter_min_cat_n(df_categorical, n=10)\n",
    "\n",
    "    # QC to binary fields with more than 2 values\n",
    "    ls_cat_fields = []\n",
    "    categorical_columns = df_categorical.select_dtypes('category').columns\n",
    "    for col in categorical_columns:\n",
    "        # Check if the field is a global variable list and skip it\n",
    "        if col in global_vars:\n",
    "            continue\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        if len(unique_values) > 2:\n",
    "            ls_cat_fields.append(col)\n",
    "            # print(\"----- ATTENTION -----\")\n",
    "            # print(f\"{col} is categorical. Check if it is binary or Continuous\")\n",
    "            # print(f\"Values: {unique_values}\\n\")\n",
    "\n",
    "    # # Summary QC in Categorical Exposures\n",
    "    # print(f\"Started process with {len(categorical_columns)} columns\\n\")\n",
    "    # print(f\"After QC:\")\n",
    "    # igem.epc.describe.summarize(df_categorical) \n",
    "    \n",
    "    #  TODO: to comment or descomment a blok of line (cmd + /)\n",
    "    \n",
    "\n",
    "    ### QC in Continuous Exposures\n",
    "    df_continuous = igem.epc.modify.colfilter_percent_zero(df_continuous, filter_percent=90)\n",
    "    df_continuous = igem.epc.modify.colfilter_min_n(df_continuous, n=200)\n",
    "\n",
    "    # # Summary QC in Continuous Exposures\n",
    "    # print(f\"Started process with {len(continuous_columns)} columns\\n\")\n",
    "    # print(f\"After QC:\")\n",
    "    # igem.epc.describe.summarize(df_continuous)\n",
    "    \n",
    "\n",
    "    # Combine filtered categorical and continuous columns\n",
    "    df_variants = pd.concat([df_categorical, df_continuous], axis=1)\n",
    "    igem.epc.describe.summarize(df_variants)\n",
    "\n",
    "\n",
    "    return df_variants, ls_cat_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 05_04_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge functions\n",
    "def merge_dfs(left, right):\n",
    "    return left.merge(right, on='ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 05_05: PROCESS QC by Phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to process each Outcome\n",
    "results = []\n",
    "var_check_list = []\n",
    "\n",
    "# list_outcomes = ['LBXTC_N',] # DEBUG ONLY/\n",
    "\n",
    "for outcome in list_outcomes:\n",
    "\n",
    "    print(f\"Start QC to {outcome}\")\n",
    "\n",
    "    # Split Datasets - Discovery and Replication\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\")\n",
    "    print(f\"----- Start Split the Datasets to {outcome} ------\")\n",
    "    \n",
    "    if outcome in ['LBDLDL_N', 'LBXTC_N', 'LBXSTR']:\n",
    "        df_discovery, df_replication = split_datasets(\n",
    "            df_nhanes_gt,\n",
    "            outcome,\n",
    "            list_covariats,\n",
    "            list_variants,\n",
    "            split_col='Cycle',\n",
    "            split_value='2007-2008' # this cicle will keep in the discovery dataset\n",
    "            )\n",
    "    elif outcome in ['LBDHDL']:\n",
    "        df_discovery, df_replication = split_datasets(\n",
    "            df_nhanes_gt,\n",
    "            outcome,\n",
    "            list_covariats,\n",
    "            list_variants,\n",
    "            split_col='Cycle',\n",
    "            split_value='1999-2000'\n",
    "            )\n",
    "    elif outcome in ['LBDHDD']:\n",
    "        df_discovery, df_replication = split_datasets(\n",
    "            df_nhanes_gt,\n",
    "            outcome,\n",
    "            list_covariats,\n",
    "            list_variants,\n",
    "            split_col='Cycle',\n",
    "            split_value='2011-2012'\n",
    "            )\n",
    "    # LBXHDD will be split randomly (has just one cycle)\n",
    "    elif outcome in ['LBXHDD']:\n",
    "        df_discovery, df_replication = split_datasets(\n",
    "            df_nhanes_gt,\n",
    "            outcome,\n",
    "            list_covariats,\n",
    "            list_variants,\n",
    "            random_split=True,\n",
    "            discovery_percent=0.7\n",
    "            )\n",
    "    \n",
    "    # Raise error if got split problems\n",
    "    if df_discovery is None or df_replication is None:\n",
    "        print(f\"Failed to split datasets for outcome: {outcome}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Apply the Outcome QC\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\n\")\n",
    "    print(f\"----- Start Outcome QC to {outcome} ------\")\n",
    "    # df_outcome_discovery = df_discovery[[outcome]]\n",
    "    # df_outcome_replication = df_replication[[outcome]]\n",
    "    print(f\"QC to Discovery Outcome Dataset\")\n",
    "    df_outcome_discovery = outcome_qc(\n",
    "        df_discovery,\n",
    "        outcome,\n",
    "        group='discovery',\n",
    "        plot=False\n",
    "        )\n",
    "    print(f\"QC to Replicate Outcome Dataset\")\n",
    "    df_outcome_replication = outcome_qc(\n",
    "        df_replication,\n",
    "        outcome,\n",
    "        group='replication',\n",
    "        plot=False\n",
    "        )\n",
    "\n",
    "    # Check Outcome QC Results\n",
    "    if df_outcome_discovery is None or df_outcome_replication is None:\n",
    "        print(f\"outcome {outcome} removed due to high missing values.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    ### Apply the Exposure QC\n",
    "    # -------------------------------------------------------------------------\n",
    "    # \n",
    "    print(\"\\n\")\n",
    "    print(f\"----- Start Variants QC to {outcome} ------\")\n",
    "    print(f\"QC to Discovery Variants Dataset\")\n",
    "    df_variants_discovery, ls_cat = variant_qc(\n",
    "        df_discovery,\n",
    "        list_variants,\n",
    "        )\n",
    "    var_check_list.extend(ls_cat)\n",
    "\n",
    "    print(f\"QC to Replicate Variants Dataset\")\n",
    "    df_variants_replication, ls_cat_rep = variant_qc(\n",
    "        df_replication,\n",
    "        list_variants,\n",
    "        )\n",
    "    var_check_list.extend(ls_cat_rep)\n",
    "\n",
    "\n",
    "    check_integrid, df_variants_discovery, df_variants_replication = sync_columns(\n",
    "        df_variants_discovery,\n",
    "        df_variants_replication\n",
    "        )\n",
    "\n",
    "\n",
    "    if not check_integrid:\n",
    "        raise ColumnSyncError(\"Column synchronization in variants failed between discovery and replication datasets.\")\n",
    "\n",
    "    print(f\"End QC to {outcome}\")\n",
    "\n",
    "    # Apply the Covariants QC\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_covariants_discovery = df_discovery[list_covariats]\n",
    "    df_covariants_replication = df_replication[list_covariats]\n",
    "\n",
    "\n",
    "    # Aggregation QC data\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Merge Discovery Data\n",
    "    dfs_to_merge = [\n",
    "        df_outcome_discovery,\n",
    "        df_covariants_discovery,\n",
    "        df_variants_discovery\n",
    "        ]\n",
    "    df_qc_discovery = reduce(merge_dfs, dfs_to_merge)\n",
    "\n",
    "    # Merge Replication Data\n",
    "    dfs_to_merge_replication = [\n",
    "        df_outcome_replication,\n",
    "        df_covariants_replication,\n",
    "        df_variants_replication\n",
    "        ]\n",
    "    df_qc_replication = reduce(merge_dfs, dfs_to_merge_replication)\n",
    "\n",
    "    # Add group column\n",
    "    df_qc_discovery['group'] = 'discovery'\n",
    "    df_qc_replication['group'] = 'replication'\n",
    "\n",
    "    # Concat discovery and replication dataframes\n",
    "    df_combined = pd.concat([df_qc_discovery, df_qc_replication])\n",
    "    df_combined = igem.epc.modify.make_categorical(df_combined, only=[\"group\",])\n",
    "    \n",
    "    # Check if any columns is Object\n",
    "    # Information about the columns not categorized and need to be set manually\n",
    "    ls_object_columns = df_combined.select_dtypes(include=['object']).columns\n",
    "    if len(ls_object_columns) != 0:\n",
    "        print(\n",
    "            f\"Number of object columns:  {len(ls_object_columns)} \\n\" \n",
    "            )\n",
    "        print(\"The list of object columns: \")\n",
    "        print(f\"{ls_object_columns} \\n\")\n",
    "        igem.epc.describe.summarize(df_combined)\n",
    "        print(\" --- Solve this before save the data ---\")\n",
    "    else:\n",
    "        # Save as Parquet to keep the data types\n",
    "        file_name = str(path_data) + \"/step_05_05/QC_NHANES_\" + outcome + \".pkl\"\n",
    "        df_combined.to_pickle(file_name)\n",
    "        print(f\"QC Data saved in {file_name}\\n\\n\")\n",
    "\n",
    "var_check_list = list(set(var_check_list))\n",
    "print(f\"var to check: {var_check_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMD415A',\n",
       " 'SMQ770',\n",
       " 'DBQ235B',\n",
       " 'DBQ235A',\n",
       " 'SMQ800',\n",
       " 'DBD197',\n",
       " 'LBDVMELC',\n",
       " 'LBDVDBLC',\n",
       " 'DMDHHSZA',\n",
       " 'DBQ235C',\n",
       " 'GTXDRANK']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_check_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "outcome = 'LBXTC_N'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Discovery and Replicate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> 274 columns and 24836 rows on DF1 dataset\n",
      "---> 274 columns and 27023 rows on DF2 dataset\n"
     ]
    }
   ],
   "source": [
    "df_discovery, df_replication = split_datasets(\n",
    "        df_nhanes_gt,\n",
    "        outcome,\n",
    "        list_covariats,\n",
    "        list_variants,\n",
    "        split_col='Cycle',\n",
    "        split_value='2007-2008'\n",
    ")\n",
    "if df_discovery is None or df_replication is None:\n",
    "    print(f\"Failed to split datasets for outcome: {outcome}\")\n",
    "    raise ValueError(\"Failed to split datasets for outcome: {outcome}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC to the Phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcome_discovery = df_discovery[[outcome]]\n",
    "df_outcome_replication = df_replication[[outcome]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running colfilter_percent_zero\n",
      "--------------------------------------------------------------------------------\n",
      "Testing 1 of 1 continuous variables\n",
      "\tRemoved 0 (0.00%) tested continuous variables which were equal to zero in at least 90.00% of non-NA observations.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "df = igem.epc.modify.colfilter_percent_zero(df_outcome_discovery, filter_percent=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The skew ratio is: 0.9532952387225929\n"
     ]
    }
   ],
   "source": [
    "skew_value = igem.epc.describe.skewness(\n",
    "    df,\n",
    "    dropna='True'\n",
    "    ).loc[0, 'skew']\n",
    "print(f'The skew ratio is: {skew_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running transform\n",
      "--------------------------------------------------------------------------------\n",
      "Transformed 'LBXTC_N' using 'log'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if abs(skew_value) > 0.5:\n",
    "    group = 'discovery'\n",
    "    df_log = igem.epc.modify.transform(df, 'log')\n",
    "    \n",
    "    # Run skewness again after log transf\n",
    "    skew_value_log = igem.epc.describe.skewness(\n",
    "        df_log,\n",
    "        dropna='True'\n",
    "    ).loc[0, 'skew']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcome = df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC to the Exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'discovery'\n",
    "plot=False\n",
    "n_min=200\n",
    "df = df_discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "# Filtering the variant columns that exist in the discovery DataFrame\n",
    "existing_variants = [var for var in list_variants if var in df.columns]\n",
    "df_variants = df[existing_variants]\n",
    "print(len(df_variants.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Number of object columns:  0\n",
      "---> Number of categorical columns:  157\n",
      "---> Number of continuous columns:  111\n"
     ]
    }
   ],
   "source": [
    "# Count the number of object columns\n",
    "objecy_columns = df_variants.select_dtypes(include=['object']).columns\n",
    "categorical_columns = df_variants.select_dtypes(include=['category']).columns\n",
    "continuous_columns = df_variants.select_dtypes(include=['number']).columns\n",
    "print(f\"---> Number of object columns:  {len(objecy_columns)}\")\n",
    "print(f\"---> Number of categorical columns:  {len(categorical_columns)}\")\n",
    "print(f\"---> Number of continuous columns:  {len(continuous_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Number of categorical columns:  157\n",
      "---> Number of continuous columns:  111\n"
     ]
    }
   ],
   "source": [
    "# Separate categorical and continuous variables\n",
    "df_categorical = df_variants[categorical_columns]\n",
    "df_continuou = df_variants[continuous_columns]\n",
    "print(f\"---> Number of categorical columns:  {len(df_categorical.columns)}\")\n",
    "print(f\"---> Number of continuous columns:  {len(df_continuou.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24,836 observations of 157 variables\n",
      "\t72 Binary Variables\n",
      "\t61 Categorical Variables\n",
      "\t0 Continuous Variables\n",
      "\t0 Unknown-Type Variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the diff 157 - 133 (72 Bin + 61 Cat) = 24 are constants columns\n",
    "igem.epc.describe.summarize(df_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC To Categorical Exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running colfilter_min_cat_n\n",
      "--------------------------------------------------------------------------------\n",
      "Testing 72 of 72 binary variables\n",
      "\tRemoved 40 (55.56%) tested binary variables which had a category with less than 100 values.\n",
      "Testing 61 of 61 categorical variables\n",
      "\tRemoved 60 (98.36%) tested categorical variables which had a category with less than 100 values.\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFor Instance, the field SMQ020 was dropped\\n1 - 10,855\\n2 - 11,692\\n3 - 3\\n9 - 18\\nBlank - 2,268\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categorical = igem.epc.modify.colfilter_min_cat_n(df_categorical, n=100)\n",
    "\"\"\"\n",
    "For Instance, the field SMQ020 was dropped\n",
    "1 - 10,855\n",
    "2 - 11,692\n",
    "3 - 3\n",
    "9 - 18\n",
    "Blank - 2,268\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running colfilter_min_n\n",
      "--------------------------------------------------------------------------------\n",
      "Testing 32 of 32 binary variables\n",
      "\tRemoved 0 (0.00%) tested binary variables which had less than 200 non-null values.\n",
      "Testing 1 of 1 categorical variables\n",
      "\tRemoved 0 (0.00%) tested categorical variables which had less than 200 non-null values.\n",
      "Testing 0 of 0 continuous variables\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "df_categorical = igem.epc.modify.colfilter_min_n(df_categorical, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started process with 157 columns\n",
      "\n",
      "After QC:\n",
      "24,836 observations of 57 variables\n",
      "\t32 Binary Variables\n",
      "\t1 Categorical Variables\n",
      "\t0 Continuous Variables\n",
      "\t0 Unknown-Type Variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary QC in Categorical Exposures\n",
    "print(f\"Started process with {len(categorical_columns)} columns\\n\")\n",
    "print(f\"After QC:\")\n",
    "igem.epc.describe.summarize(df_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field: SMD415A\n",
      "Values: [2.0, 1.0, 3.0]\n",
      "Categories (3, float64): [1.0, 2.0, 3.0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrerico/Works/Projects/pbs_igem/.venv/lib/python3.10/site-packages/pandas/io/formats/format.py:1429: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  for val, m in zip(values.ravel(), mask.ravel())\n"
     ]
    }
   ],
   "source": [
    "# QC to binary fields with more than 2 values\n",
    "categorical_columns = df_categorical.select_dtypes('category').columns\n",
    "\n",
    "for col in categorical_columns:\n",
    "    # get unique values\n",
    "    # unique_values = df[col].unique()\n",
    "    # unique_values = pd.Series(df[col].unique()).dropna().values\n",
    "    unique_values = df[col].dropna().unique()\n",
    "\n",
    "    # Check if some categorical is Binary\n",
    "    if len(unique_values) > 2:\n",
    "        print(f\"Field: {col}\")\n",
    "        print(f\"Values: {unique_values}\\n\")\n",
    "\n",
    "    \n",
    "#     # Verificar se a coluna contém os valores 1, 2, 7 e 9\n",
    "#     if set([1, 2, 7, 9]).issubset(unique_values):\n",
    "#         # Substituir valores 7 e 9 por np.nan\n",
    "#         df[col] = df[col].replace({7: np.nan, 9: np.nan})\n",
    "\n",
    "\n",
    "# # Aplicar a função ao DataFrame df_categorical\n",
    "# df_categorical_cleaned = clean_categorical_columns(df_categorical)\n",
    "\n",
    "# # Exibir o DataFrame limpo\n",
    "# print(df_categorical_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC to Continuous Exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running colfilter_percent_zero\n",
      "--------------------------------------------------------------------------------\n",
      "Testing 111 of 111 continuous variables\n",
      "\tRemoved 0 (0.00%) tested continuous variables which were equal to zero in at least 90.00% of non-NA observations.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "df_continuous = igem.epc.modify.colfilter_percent_zero(df_continuou, filter_percent=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started process with 111 columns\n",
      "\n",
      "After QC:\n",
      "24,836 observations of 111 variables\n",
      "\t0 Binary Variables\n",
      "\t0 Categorical Variables\n",
      "\t111 Continuous Variables\n",
      "\t0 Unknown-Type Variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary QC in Continuous Exposures\n",
    "print(f\"Started process with {len(continuous_columns)} columns\\n\")\n",
    "print(f\"After QC:\")\n",
    "igem.epc.describe.summarize(df_continuou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge QC Exposures Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24,836 observations of 168 variables\n",
      "\t32 Binary Variables\n",
      "\t1 Categorical Variables\n",
      "\t111 Continuous Variables\n",
      "\t0 Unknown-Type Variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine filtered categorical and continuous columns\n",
    "df_variants = pd.concat([df_categorical, df_continuous], axis=1)\n",
    "igem.epc.describe.summarize(df_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC to Covariantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24,836 observations of 5 variables\n",
      "\t1 Binary Variables\n",
      "\t2 Categorical Variables\n",
      "\t2 Continuous Variables\n",
      "\t0 Unknown-Type Variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_covariants = df_discovery[list_covariats]\n",
    "igem.epc.describe.summarize(df_covariants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all QC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24,836 observations of 174 variables\n",
      "\t33 Binary Variables\n",
      "\t3 Categorical Variables\n",
      "\t114 Continuous Variables\n",
      "\t0 Unknown-Type Variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create list of QC dataframes\n",
    "dfs_to_merge = [\n",
    "    df_outcome,\n",
    "    df_covariants,\n",
    "    df_variants\n",
    "    ]\n",
    "df_qc = reduce(merge_dfs, dfs_to_merge)\n",
    "igem.epc.describe.summarize(df_qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running make_categorical\n",
      "--------------------------------------------------------------------------------\n",
      "Set 1 of 175 variable(s) as categorical, each with 24,836 observations\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Add group column\n",
    "df_qc['group'] = 'discovery'\n",
    "df_qc = igem.epc.modify.make_categorical(df_qc, only=[\"group\",])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
